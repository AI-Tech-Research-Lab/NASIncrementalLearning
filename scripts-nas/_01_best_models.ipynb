{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fsahli/vvcastro/continual-nas\n"
     ]
    }
   ],
   "source": [
    "import _config\n",
    "\n",
    "%cd ..\n",
    "\n",
    "\n",
    "from tools.post_search import (\n",
    "    get_archive_best_models, get_base_model_size, get_model_flatness, get_best_archive,\n",
    ")\n",
    "from continual_learning.continual_metrics import compute_continual_metrics\n",
    "from search_space import get_search_space, ModelSample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "BASE_DIR = \"nas-results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an experiment and retrieve all the seed-values available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cifar10\"\n",
    "MODEL_TYPE = \"growing\"\n",
    "\n",
    "NAS_TYPE = \"multi-objective\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions to retrieve the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_best_archives(dataset: str, model_type: str):\n",
    "\n",
    "    # Get the experiments matching the dataset and model type\n",
    "    _basedir = os.path.join(BASE_DIR, f\"efficient-{dataset}\")\n",
    "    experiments_dirs = [ dir for dir in os.listdir(_basedir) if f\"{model_type}-seed\" in dir ]\n",
    "\n",
    "    # Get the seeds available for each experiment\n",
    "    seeds = [ int(re.search(r\"\\d+\", dir).group()) for dir in experiments_dirs ]\n",
    "\n",
    "    # Load the the last archive for each experiment\n",
    "    last_archives, archive_seeds = [], []\n",
    "\n",
    "    for seed in seeds:\n",
    "        try:\n",
    "            seed_archive = get_best_archive(\n",
    "                BASE_DIR, dataset, model_type, seed, nas_type=NAS_TYPE,\n",
    "            )\n",
    "            last_archives.append(seed_archive)\n",
    "            archive_seeds.append(seed)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    return last_archives, archive_seeds\n",
    "\n",
    "def get_best_models_from_archive(\n",
    "    archive: dict,\n",
    "    seed: int,\n",
    "    sort_by: str = \"accuracy\",\n",
    "):\n",
    "\n",
    "    archive_models = [datum[\"sample\"] for datum in archive]\n",
    "    archive_metrics = np.stack([datum[\"metrics\"] for datum in archive])\n",
    "\n",
    "    # Get the best models from the archive\n",
    "    best_models, best_metrics = get_archive_best_models(\n",
    "        archive_models,\n",
    "        archive_metrics,\n",
    "        n_best=50\n",
    "    )\n",
    "\n",
    "    if NAS_TYPE == \"single-objective\":\n",
    "        flatnesses = []\n",
    "        for model in best_models:\n",
    "            model_flatness = get_model_flatness(\n",
    "                BASE_DIR, DATASET_NAME, MODEL_TYPE, seed, model\n",
    "            )\n",
    "            flatnesses.append(model_flatness)\n",
    "\n",
    "        # Concatenate the flatness to the metrics\n",
    "        best_metrics = np.concatenate(\n",
    "            [best_metrics, np.array(flatnesses).reshape(-1, 1)],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    # Get the model with the best accuracy\n",
    "    sort_idx = 0 if sort_by == \"accuracy\" else 1\n",
    "    best_accuracy_idx = np.argmax(best_metrics[:, sort_idx])\n",
    "    best_accuracy_model = best_models[best_accuracy_idx]\n",
    "    return best_accuracy_model, best_metrics[best_accuracy_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the seeds and aggregate to get the final model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 archives\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Mean: 96.51564079214234\n",
      "Std: 0.22541629431276003\n",
      "--------------------------------------------------\n",
      "Flatness:\n",
      "Mean: 76.0812767184703\n",
      "Std: 3.37799548459081\n",
      "--------------------------------------------------\n",
      "Model size:\n",
      "Mean: 3.9233333333333333\n",
      "Std: 0.498820831784542\n"
     ]
    }
   ],
   "source": [
    "SORT_BY = \"accuracy\"\n",
    "\n",
    "last_archives, seeds = get_experiment_best_archives(DATASET_NAME, MODEL_TYPE)\n",
    "print(f\"Found {len(seeds)} archives\")\n",
    "\n",
    "# Store the best accuracies and flatnesses\n",
    "best_accuracies, best_flatnesses = [], []\n",
    "bast_params = []\n",
    "\n",
    "best_models = []\n",
    "for archive, seed in zip(last_archives, seeds):\n",
    "    best_model, best_metrics = get_best_models_from_archive(\n",
    "        archive,\n",
    "        seed,\n",
    "        sort_by=SORT_BY\n",
    "    )\n",
    "\n",
    "    # Aggregate accuracies\n",
    "    best_accuracies.append(best_metrics[0])\n",
    "    best_flatnesses.append(best_metrics[1])\n",
    "\n",
    "    # Add the model's size\n",
    "    bast_params.append( get_base_model_size(DATASET_NAME, best_model) )\n",
    "    best_models.append(best_model)\n",
    "\n",
    "# Compute the mean and std\n",
    "print(\"Accuracy:\")\n",
    "print(f\"Mean: {np.mean(best_accuracies)}\")\n",
    "print(f\"Std: {np.std(best_accuracies)}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Flatness:\")\n",
    "print(f\"Mean: {np.mean(best_flatnesses)}\")\n",
    "print(f\"Std: {np.std(best_flatnesses)}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Model size:\")\n",
    "print(f\"Mean: {np.mean(bast_params)}\")\n",
    "print(f\"Std: {np.std(bast_params)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate to get the continual metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def test_model(test_dir: str, model: ModelSample, model_type: str, seed: int, n_tasks: int = 10):\n",
    "    search_space = get_search_space(\"mobilenetv3\", model_type == \"fixed\")\n",
    "\n",
    "    # Build the evaluator run params\n",
    "    model_encoding = search_space.encode(model)\n",
    "    params = [\n",
    "        \"python\",\n",
    "        \"scripts-nas/02_model_evaluator.py\",\n",
    "        \"--experiment_dir\",\n",
    "        str(test_dir),\n",
    "        \"--model_encoding\",\n",
    "        *map(str, model_encoding),\n",
    "        \"--dataset\",\n",
    "        str(DATASET_NAME),\n",
    "        \"--n_tasks\",\n",
    "        str(n_tasks),\n",
    "        \"--epochs_per_task\",\n",
    "        \"1\",\n",
    "        \"--architecture\",\n",
    "        str(\"expandable\" if model_type == \"growing\" else \"fixed\"),\n",
    "        \"--random_seed\",\n",
    "        str(seed),\n",
    "    ]\n",
    "\n",
    "    # Execute the script\n",
    "    subprocess.run(params, check=True)\n",
    "    return model_encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models in both 5 and 10 tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1100010101001010100010011001110010101011010109010-44 already trained, skipping...\n",
      "Model 1100010101001010100010011001110010101011010109010-44 already trained, skipping...\n",
      "Model 1011001001110001100100001000010010000000000004010-43 already trained, skipping...\n",
      "Model 1011001001110001100100001000010010000000000004010-43 already trained, skipping...\n",
      "Model 1010011101110000001111001001111001000100001005010-42 already trained, skipping...\n",
      "Model 1010011101110000001111001001111001000100001005010-42 already trained, skipping...\n"
     ]
    }
   ],
   "source": [
    "BEST_MODELS_RESULTS_DIR = os.path.join(\n",
    "    \"nas-tester\",\n",
    "    f\"efficient-{DATASET_NAME}\",\n",
    "    \"best-models\",\n",
    ")\n",
    "\n",
    "evaluated_experiments = []\n",
    "for model, seed in zip(best_models, seeds):\n",
    "    encoded_model = test_model(BEST_MODELS_RESULTS_DIR, model, MODEL_TYPE, seed, n_tasks=5)\n",
    "    test_model(BEST_MODELS_RESULTS_DIR, model, MODEL_TYPE, seed, n_tasks=10)\n",
    "    evaluated_experiments.append(\n",
    "        \"\".join(map(str, encoded_model)) + f\"-{seed}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TASKS = 10\n",
    "\n",
    "# Tasks models dir\n",
    "experiments_base_dir = os.path.join(BEST_MODELS_RESULTS_DIR + f\"-{N_TASKS}\", \"models\")\n",
    "\n",
    "# Get all the evaluated experiments\n",
    "experiment_metrics = []\n",
    "\n",
    "for experiment_dir in evaluated_experiments:\n",
    "    experiment_path = os.path.join(experiments_base_dir, experiment_dir)\n",
    "    history_path = os.path.join(experiment_path, \"history.json\")\n",
    "\n",
    "    # Read the training history file\n",
    "    with open(history_path, \"r\") as f:\n",
    "        model_history = json.load(f)\n",
    "\n",
    "    # Get the metrics from the history\n",
    "    model_metrics = compute_continual_metrics(model_history)\n",
    "    model_flatness = [\n",
    "        t[\"flatness\"][-1]\n",
    "        for t in model_history[\"training_metrics\"][\"validation\"].values()\n",
    "    ]\n",
    "    model_metrics[\"flatness\"] = np.array([np.mean(model_flatness)])\n",
    "\n",
    "    experiment_metrics.append(model_metrics)\n",
    "\n",
    "\n",
    "# Group the metrics by mean and std\n",
    "metric_names = experiment_metrics[0].keys()\n",
    "aggregated_metrics = {}\n",
    "for metric in metric_names: \n",
    "    aggregated_metrics[metric] = np.array([\n",
    "        exp_metrics[metric] for exp_metrics in experiment_metrics\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mean and std of the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_accuracy\n",
      "Mean: 94.6732254069356\n",
      "Std: 0.9028246266415635\n",
      "--------------------------------------------------\n",
      "average_incremental_accuracy\n",
      "Mean: 94.46292320887247\n",
      "Std: 0.8542733600195338\n",
      "--------------------------------------------------\n",
      "average_forgetting\n",
      "Mean: 2.3856570944190025\n",
      "Std: 0.3408280601027133\n",
      "--------------------------------------------------\n",
      "backward_transfer\n",
      "Mean: -2.3560273649445618\n",
      "Std: 0.30339583257549346\n",
      "--------------------------------------------------\n",
      "forward_transfer\n",
      "Mean: 0.6043721003381298\n",
      "Std: 0.2969991704659652\n",
      "--------------------------------------------------\n",
      "flatness\n",
      "Mean: 80.5379482632305\n",
      "Std: 2.1256914424982973\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for metric in aggregated_metrics:\n",
    "    print(metric)\n",
    "    print(\"Mean:\", 100 * np.nanmean(aggregated_metrics[metric], axis=0)[-1])\n",
    "    print(\"Std:\", 100 * np.nanstd(aggregated_metrics[metric], axis=0)[-1])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
