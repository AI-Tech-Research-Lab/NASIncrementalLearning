{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _baseline_utils import get_search_space\n",
    "\n",
    "from continual_learning.continual_metrics import compute_continual_metrics\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the experiments for a given input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cifar10\"\n",
    "N_TASKS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File results-random@5/cifar10/models/ewc-seed43/history.json not found\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "results_dirs = os.path.join(\n",
    "    f\"results-random@{N_TASKS}\",\n",
    "    DATASET_NAME,\n",
    "    \"models\"\n",
    ")\n",
    "\n",
    "# Group the experiment by the baseline name\n",
    "experiments_groups = defaultdict(lambda: defaultdict(list))\n",
    "for baseline_model in os.listdir(results_dirs):\n",
    "    baseline_method, seed = baseline_model.split(\"-\")\n",
    "\n",
    "    # Read models data from the history\n",
    "    baseline_history_path = os.path.join(results_dirs, baseline_model, \"history.json\")\n",
    "    try:\n",
    "        with open(baseline_history_path, \"r\") as f:\n",
    "            baseline_history = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {baseline_history_path} not found\")\n",
    "        continue\n",
    "\n",
    "    # Compute the continual metrics\n",
    "    if ( baseline_method == \"joint\"):\n",
    "        training_metrics = baseline_history[\"joint_training\"][\"validation\"]\n",
    "        baseline_accuracy = [\n",
    "            btask[\"accuracy\"][-1] for btask in training_metrics.values()\n",
    "        ]\n",
    "        baseline_flatness = [\n",
    "            btask[\"flatness\"][-1] for btask in training_metrics.values()\n",
    "        ]\n",
    "\n",
    "        # Add the data for the data dict\n",
    "        experiments_groups[baseline_method][\"average_accuracy\"].append(\n",
    "            baseline_accuracy\n",
    "        )\n",
    "\n",
    "        experiments_groups[baseline_method][\"flatness\"].append(\n",
    "            baseline_flatness\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        continual_metrics = compute_continual_metrics(baseline_history)\n",
    "        training_metrics = baseline_history[\"training_metrics\"][\"validation\"]\n",
    "        baseline_accuracy = [\n",
    "            btask[\"accuracy\"][-1] for btask in training_metrics.values()\n",
    "        ]\n",
    "        baseline_flatness = [\n",
    "            btask[\"flatness\"][-1] for btask in training_metrics.values()\n",
    "        ]\n",
    "\n",
    "        # Add the data for the data dict\n",
    "        for metric in continual_metrics:\n",
    "            experiments_groups[baseline_method][metric].append(continual_metrics[metric])\n",
    "        \n",
    "        # Add the flatness\n",
    "        experiments_groups[baseline_method][\"flatness\"].append(\n",
    "            baseline_flatness\n",
    "        )\n",
    "\n",
    "# Aggregate the data for group\n",
    "baseline_groups = list(experiments_groups.keys())\n",
    "\n",
    "for baseline_group in baseline_groups:\n",
    "    for metric in experiments_groups[baseline_group]:\n",
    "        metric_mean = np.nanmean(experiments_groups[baseline_group][metric], axis=0)\n",
    "        metric_std = np.nanstd(experiments_groups[baseline_group][metric], axis=0)\n",
    "        experiments_groups[baseline_group][metric] = {\n",
    "            \"mean\": metric_mean,\n",
    "            \"std\": metric_std\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the accuracies for each baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_accuracy\n",
      "Mean:  95.98166666666668\n",
      "Std:  0.0\n",
      "\n",
      "average_incremental_accuracy\n",
      "Mean:  95.10244131088257\n",
      "Std:  0.0\n",
      "\n",
      "average_forgetting\n",
      "Mean:  0.9291666559875011\n",
      "Std:  0.0\n",
      "\n",
      "backward_transfer\n",
      "Mean:  -0.929166666666667\n",
      "Std:  0.0\n",
      "\n",
      "forward_transfer\n",
      "Mean:  -0.8208333333333317\n",
      "Std:  0.0\n",
      "\n",
      "flatness\n",
      "Mean:  78.24714774544577\n",
      "Std:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BASELINE_METHOD = \"ewc\"\n",
    "\n",
    "for metric in experiments_groups[BASELINE_METHOD]:\n",
    "    print(metric)\n",
    "    print(\"Mean: \", 100 * experiments_groups[BASELINE_METHOD][metric][\"mean\"][-1])\n",
    "    print(\"Std: \", 100 * experiments_groups[BASELINE_METHOD][metric][\"std\"][-1])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas-cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
